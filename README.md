<h1 align="center">ğŸ“Š Sampling Techniques & Machine Learning</h1>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8+-blue?style=for-the-badge&logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/Scikit--Learn-ML-orange?style=for-the-badge&logo=scikit-learn&logoColor=white" />
  <img src="https://img.shields.io/badge/Status-Completed-success?style=for-the-badge" />
</p>

<p align="center">
  <i>Analyzing the impact of different sampling techniques on machine learning model performance using an imbalanced credit card fraud dataset.</i>
</p>

---

## ğŸ“Œ Problem Statement

Given a **highly imbalanced credit card dataset**, the goal is to:
1. Balance the dataset using **SMOTE**
2. Apply **5 different sampling techniques**
3. Evaluate **5 different ML models**
4. Determine which sampling technique works best for each model

---

## ğŸ“‚ Dataset

| Property | Value |
|----------|-------|
| **Source** | [Credit Card Dataset](https://github.com/AnjulaMehto/Sampling_Assignment/blob/main/Creditcard_data.csv) |
| **Rows** | 772 |
| **Features** | 30 |
| **Target** | `Class` (0 = Legit, 1 = Fraud) |
| **Initial Balance** | Highly Imbalanced âš ï¸ |

---

## âš™ï¸ Methodology
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Load Data  â†’  2. Balance (SMOTE)  â†’  3. Create 5 Samples    â”‚
â”‚                                                                 â”‚
â”‚  4. Train 5 Models on Each Sample  â†’  5. Compare Accuracies     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”¹ Sampling Techniques

| # | Technique | Description |
|:-:|-----------|-------------|
| 1 | **Simple Random** | Random selection without any criteria |
| 2 | **Stratified** | Maintains class distribution ratio |
| 3 | **Systematic** | Selects every k-th element |
| 4 | **Cluster** | Divides into clusters, samples from selected cluster |
| 5 | **Bootstrap** | Random sampling with replacement |

### ğŸ”¹ Machine Learning Models

| # | Model | Type |
|:-:|-------|------|
| M1 | Logistic Regression | Linear |
| M2 | Decision Tree | Tree-based |
| M3 | Random Forest | Ensemble |
| M4 | SVM | Kernel-based |
| M5 | Gradient Boosting | Boosting |

---

## ğŸ“Š Results

### Accuracy Table (%)

| Model | Simple Random | Stratified | Systematic | Cluster | Bootstrap |
|:------|:-------------:|:----------:|:----------:|:-------:|:---------:|
| Logistic Regression | 87.07 | 91.38 | 89.66 | 86.96 | **95.69** |
| Decision Tree | 93.97 | 93.97 | **98.28** | 86.96 | 93.10 |
| Random Forest | 98.28 | 98.28 | **100.00** ğŸ† | 96.74 | 95.69 |
| SVM | 59.48 | 68.97 | **73.28** | 53.26 | 67.24 |
| Gradient Boosting | 96.55 | 95.69 | **98.28** | 93.48 | 94.83 |

---

## ğŸ† Best Sampling Technique for Each Model

| Model | Best Technique | Accuracy |
|:------|:--------------:|:--------:|
| Logistic Regression | Bootstrap | 95.69% |
| Decision Tree | Systematic | 98.28% |
| **Random Forest** | **Systematic** | **100.0%** ğŸ¥‡ |
| SVM | Systematic | 73.28% |
| Gradient Boosting | Systematic | 98.28% |

---

## ğŸ“ˆ Visualizations

<p align="center">
  <img src="images/heatmap.png" width="600" alt="Accuracy Heatmap"/>
</p>

<p align="center"><i>Heatmap showing accuracy of each model with different sampling techniques</i></p>

---

## ğŸ’¡ Key Findings
```
âœ… Random Forest + Systematic Sampling achieved 100% accuracy (Best Combination)

âœ… Systematic Sampling performed best for 4 out of 5 models

âœ… Cluster Sampling showed lowest performance across all models

âœ… SVM had the lowest overall accuracy compared to other models

âœ… Tree-based models (Decision Tree, Random Forest, Gradient Boosting) 
   consistently outperformed linear models
```

---

## ğŸ¯ Final Conclusion

| Metric | Result |
|--------|--------|
| ğŸ¥‡ **Best Model** | Random Forest |
| ğŸ¥‡ **Best Sampling Technique** | Systematic Sampling |
| ğŸ† **Best Combination** | Random Forest + Systematic (100%) |
| âš ï¸ **Worst Sampling** | Cluster Sampling |
| âš ï¸ **Worst Model** | SVM |

---

## ğŸš€ Quick Start
```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/Sampling_Assignment.git

# Install dependencies
pip install pandas numpy scikit-learn imbalanced-learn matplotlib seaborn

# Run the notebook
jupyter notebook Sampling_Assignment.ipynb
```

---

## ğŸ“ Project Structure
```
Sampling_Assignment/
â”‚
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ““ Sampling_Assignment.ipynb
â”œâ”€â”€ ğŸ“Š Creditcard_data.csv
â”œâ”€â”€ ğŸ“‹ sampling_results.csv
â”œ
â””â”€â”€ ğŸ“ images/
    â””â”€â”€ heatmap.png
```

---

## ğŸ› ï¸ Technologies Used

<p align="left">
  <img src="https://img.shields.io/badge/Python-3776AB?style=flat-square&logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/Pandas-150458?style=flat-square&logo=pandas&logoColor=white" />
  <img src="https://img.shields.io/badge/NumPy-013243?style=flat-square&logo=numpy&logoColor=white" />
  <img src="https://img.shields.io/badge/Scikit--Learn-F7931E?style=flat-square&logo=scikit-learn&logoColor=white" />
  <img src="https://img.shields.io/badge/Matplotlib-11557C?style=flat-square&logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/Seaborn-3776AB?style=flat-square&logo=python&logoColor=white" />
</p>

---

## ğŸ‘¤ Author

<h3>Prabhleen Kaur</h3>

[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/prabhleen003)

---

<p align="center">
  <b>â­ Star this repository if you found it helpful!</b>
</p>
